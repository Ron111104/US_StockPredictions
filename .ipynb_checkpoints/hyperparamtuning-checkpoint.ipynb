{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1e1a7b-c382-4b0c-b56c-b57a92b64464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ta  # Install via: pip install ta\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, LSTM, Dense, Concatenate, TimeDistributed,Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097fbd61-9e71-47a4-abf6-32ef5c46fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired date range\n",
    "start_date_str = \"02/01/2018\"  # 2nd January 2003 (DD-MM-YYYY)\n",
    "end_date_str   = \"28/02/2025\"  # 28th February 2025 (DD-MM-YYYY)\n",
    "start_date_filter = pd.to_datetime(start_date_str, dayfirst=True)\n",
    "end_date_filter   = pd.to_datetime(end_date_str, dayfirst=True)\n",
    "raw_data_dir = \"./data\"           # Folder with raw CSV files (e.g., AAPL.csv, SPY.csv)\n",
    "filtered_data_dir = \"./filtered_data_lstmgru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f71e37-24d1-4b04-8020-05af80dbdc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL.csv - Date Range: 1980-12-12 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed AAPL.csv and saved to ./filtered_data_lstmgru\\AAPL.csv\n",
      "ABBV.csv - Date Range: 2013-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed ABBV.csv and saved to ./filtered_data_lstmgru\\ABBV.csv\n",
      "ADBE.csv - Date Range: 1986-08-13 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed ADBE.csv and saved to ./filtered_data_lstmgru\\ADBE.csv\n",
      "AMD.csv - Date Range: 1980-03-17 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed AMD.csv and saved to ./filtered_data_lstmgru\\AMD.csv\n",
      "AMT.csv - Date Range: 1998-02-27 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed AMT.csv and saved to ./filtered_data_lstmgru\\AMT.csv\n",
      "AMZN.csv - Date Range: 1997-05-15 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed AMZN.csv and saved to ./filtered_data_lstmgru\\AMZN.csv\n",
      "BA.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed BA.csv and saved to ./filtered_data_lstmgru\\BA.csv\n",
      "BAC.csv - Date Range: 1973-02-21 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed BAC.csv and saved to ./filtered_data_lstmgru\\BAC.csv\n",
      "BLK.csv - Date Range: 1999-10-01 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed BLK.csv and saved to ./filtered_data_lstmgru\\BLK.csv\n",
      "CAT.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed CAT.csv and saved to ./filtered_data_lstmgru\\CAT.csv\n",
      "COP.csv - Date Range: 1981-12-31 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed COP.csv and saved to ./filtered_data_lstmgru\\COP.csv\n",
      "CRM.csv - Date Range: 2004-06-23 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed CRM.csv and saved to ./filtered_data_lstmgru\\CRM.csv\n",
      "CVX.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed CVX.csv and saved to ./filtered_data_lstmgru\\CVX.csv\n",
      "DUK.csv - Date Range: 1980-03-17 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed DUK.csv and saved to ./filtered_data_lstmgru\\DUK.csv\n",
      "FCX.csv - Date Range: 1995-07-10 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed FCX.csv and saved to ./filtered_data_lstmgru\\FCX.csv\n",
      "GOOGL.csv - Date Range: 2004-08-19 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed GOOGL.csv and saved to ./filtered_data_lstmgru\\GOOGL.csv\n",
      "GS.csv - Date Range: 1999-05-04 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed GS.csv and saved to ./filtered_data_lstmgru\\GS.csv\n",
      "HD.csv - Date Range: 1981-09-22 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed HD.csv and saved to ./filtered_data_lstmgru\\HD.csv\n",
      "HON.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed HON.csv and saved to ./filtered_data_lstmgru\\HON.csv\n",
      "JNJ.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed JNJ.csv and saved to ./filtered_data_lstmgru\\JNJ.csv\n",
      "JPM.csv - Date Range: 1980-03-17 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed JPM.csv and saved to ./filtered_data_lstmgru\\JPM.csv\n",
      "LIN.csv - Date Range: 1992-06-17 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed LIN.csv and saved to ./filtered_data_lstmgru\\LIN.csv\n",
      "META.csv - Date Range: 2012-05-18 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed META.csv and saved to ./filtered_data_lstmgru\\META.csv\n",
      "MRK.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed MRK.csv and saved to ./filtered_data_lstmgru\\MRK.csv\n",
      "MSFT.csv - Date Range: 1986-03-13 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed MSFT.csv and saved to ./filtered_data_lstmgru\\MSFT.csv\n",
      "NEE.csv - Date Range: 1973-02-21 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed NEE.csv and saved to ./filtered_data_lstmgru\\NEE.csv\n",
      "NVDA.csv - Date Range: 1999-01-22 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed NVDA.csv and saved to ./filtered_data_lstmgru\\NVDA.csv\n",
      "PFE.csv - Date Range: 1972-06-01 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed PFE.csv and saved to ./filtered_data_lstmgru\\PFE.csv\n",
      "PLD.csv - Date Range: 1997-11-21 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed PLD.csv and saved to ./filtered_data_lstmgru\\PLD.csv\n",
      "SBUX.csv - Date Range: 1992-06-26 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed SBUX.csv and saved to ./filtered_data_lstmgru\\SBUX.csv\n",
      "SPY.csv - Date Range: 1993-01-29 00:00:00 to 2025-02-28 00:00:00\n",
      "Processed SPY.csv and saved to ./filtered_data_lstmgru\\SPY.csv\n",
      "TSLA.csv - Date Range: 2010-06-29 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed TSLA.csv and saved to ./filtered_data_lstmgru\\TSLA.csv\n",
      "XOM.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed XOM.csv and saved to ./filtered_data_lstmgru\\XOM.csv\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "os.makedirs(filtered_data_dir, exist_ok=True)\n",
    "\n",
    "def calculate_tema(series, window):\n",
    "    \"\"\"\n",
    "    Calculate the Triple Exponential Moving Average (TEMA)\n",
    "    using the formula: TEMA = 3*EMA1 - 3*EMA2 + EMA3\n",
    "    where EMA1 = EMA(series, window)\n",
    "          EMA2 = EMA(EMA1, window)\n",
    "          EMA3 = EMA(EMA2, window)\n",
    "    \"\"\"\n",
    "    ema1 = ta.trend.EMAIndicator(close=series, window=window, fillna=False).ema_indicator()\n",
    "    ema2 = ta.trend.EMAIndicator(close=ema1, window=window, fillna=False).ema_indicator()\n",
    "    ema3 = ta.trend.EMAIndicator(close=ema2, window=window, fillna=False).ema_indicator()\n",
    "    tema = 3 * ema1 - 3 * ema2 + ema3\n",
    "    return tema\n",
    "\n",
    "def process_csv(file_path, filename):\n",
    "    try:\n",
    "        # Specific parsing based on filename\n",
    "        if filename.upper() == \"SPY.CSV\":\n",
    "            # SPY.csv is in DD-MM-YYYY format\n",
    "            df = pd.read_csv(file_path, parse_dates=[\"Date\"], dayfirst=True)\n",
    "        else:\n",
    "            # Other files are in YYYY-MM-DD format\n",
    "            df = pd.read_csv(file_path, parse_dates=[\"Date\"], dayfirst=False)\n",
    "\n",
    "        # Drop rows where Date could not be parsed\n",
    "        df.dropna(subset=[\"Date\"], inplace=True)\n",
    "\n",
    "        # Ensure Date column is datetime and remove timezone info\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "        # Verify we have the necessary columns\n",
    "        required_columns = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Warning: {filename} is missing columns: {missing_columns}\")\n",
    "            return None\n",
    "\n",
    "        # Print min and max dates for debugging\n",
    "        print(f\"{filename} - Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "\n",
    "        # Filter for dates between start and end dates\n",
    "        df_filtered = df[\n",
    "            (df[\"Date\"] >= start_date_filter) & \n",
    "            (df[\"Date\"] <= end_date_filter)\n",
    "        ].copy()\n",
    "        \n",
    "        if df_filtered.empty:\n",
    "            print(f\"Warning: {filename} has no data within the specified date range.\")\n",
    "            return None\n",
    "\n",
    "        # Retain only essential columns\n",
    "        df_filtered = df_filtered[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        \n",
    "        # --- Calculate Standard Technical Indicators ---\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"SMA_{window}\"] = ta.trend.SMAIndicator(close=df_filtered[\"Close\"], window=window, fillna=False).sma_indicator()\n",
    "            df_filtered[f\"EMA_{window}\"] = ta.trend.EMAIndicator(close=df_filtered[\"Close\"], window=window, fillna=False).ema_indicator()\n",
    "            df_filtered[f\"TEMA_{window}\"] = calculate_tema(df_filtered[\"Close\"], window)\n",
    "        \n",
    "        # Bollinger Bands (20-day window, std dev=2)\n",
    "        bb_indicator = ta.volatility.BollingerBands(close=df_filtered[\"Close\"], window=20, window_dev=2, fillna=False)\n",
    "        df_filtered[\"BB_Hband\"] = bb_indicator.bollinger_hband()\n",
    "        df_filtered[\"BB_Mband\"] = bb_indicator.bollinger_mavg()\n",
    "        df_filtered[\"BB_Lband\"] = bb_indicator.bollinger_lband()\n",
    "        \n",
    "        # RSI (14-day)\n",
    "        df_filtered[\"RSI_14\"] = ta.momentum.RSIIndicator(close=df_filtered[\"Close\"], window=14, fillna=False).rsi()\n",
    "        \n",
    "        # MACD: using default parameters (fast=12, slow=26, signal=9)\n",
    "        macd_indicator = ta.trend.MACD(close=df_filtered[\"Close\"], window_slow=26, window_fast=12, window_sign=9, fillna=False)\n",
    "        df_filtered[\"MACD\"] = macd_indicator.macd()\n",
    "        df_filtered[\"MACD_Signal\"] = macd_indicator.macd_signal()\n",
    "        df_filtered[\"MACD_Hist\"] = macd_indicator.macd_diff()\n",
    "        \n",
    "        # Derived feature: Mean_HL as the average of High and Low\n",
    "        df_filtered[\"Mean_HL\"] = (df_filtered[\"High\"] + df_filtered[\"Low\"]) / 2.0\n",
    "        \n",
    "        # --- Calculate Extra Relative & Trend-based Indicators ---\n",
    "        # Relative Momentum (RMom) for a 14-day window\n",
    "        df_filtered[\"RMom_14\"] = df_filtered[\"Close\"] / df_filtered[\"Close\"].shift(14)\n",
    "        \n",
    "        # MomTEMA: ratio of current TEMA to its previous value (offset = 1 day)\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"MomTEMA_{window}_ofs1\"] = df_filtered[f\"TEMA_{window}\"] / df_filtered[f\"TEMA_{window}\"].shift(1)\n",
    "        \n",
    "        # RCTEMA: ratio of current Close to TEMA for each window\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"RCTEMA_{window}\"] = df_filtered[\"Close\"] / df_filtered[f\"TEMA_{window}\"]\n",
    "        \n",
    "        # MomEMA: ratio of current EMA to its previous value (offset = 1 day)\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"MomEMA_{window}_ofs1\"] = df_filtered[f\"EMA_{window}\"] / df_filtered[f\"EMA_{window}\"].shift(1)\n",
    "        \n",
    "        # Ratio Indicators for a chosen fast/slow pair (example: 14 vs. 50)\n",
    "        df_filtered[\"RTEMA_TEMA_14_50\"] = df_filtered[\"TEMA_14\"] / df_filtered[\"TEMA_50\"]\n",
    "        df_filtered[\"REMA_EMA_14_50\"] = df_filtered[\"EMA_14\"] / df_filtered[\"EMA_50\"]\n",
    "        df_filtered[\"RSMA_SMA_14_50\"] = df_filtered[\"SMA_14\"] / df_filtered[\"SMA_50\"]\n",
    "        \n",
    "        # Relative Volume to SMA: compares current volume to its 20-day SMA\n",
    "        df_filtered[\"RVolSMA_20\"] = df_filtered[\"Volume\"] / df_filtered[\"Volume\"].rolling(window=20).mean()\n",
    "        \n",
    "        # Drop rows with NaN values from indicator calculations\n",
    "        df_filtered.dropna(inplace=True)\n",
    "        \n",
    "        # --- Convert all dates to ISO format (YYYY-MM-DD) ---\n",
    "        df_filtered[\"Date\"] = df_filtered[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "for filename in os.listdir(raw_data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(raw_data_dir, filename)\n",
    "        \n",
    "        # Process the file\n",
    "        processed_df = process_csv(file_path, filename)\n",
    "        \n",
    "        # Save if processing was successful\n",
    "        if processed_df is not None:\n",
    "            output_path = os.path.join(filtered_data_dir, filename)\n",
    "            processed_df.to_csv(output_path, index=False)\n",
    "            print(f\"Processed {filename} and saved to {output_path}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3169056e-b560-4b5d-9349-a77f83ad44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Part 2: Load Filtered Data and Feature Engineering for Modeling\n",
    "# =======================================================\n",
    "\n",
    "def load_csv_data(filepath):\n",
    "    \"\"\"Load CSV file and parse the Date column appropriately.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Use the correct format: ISO format (YYYY-MM-DD)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "    df.sort_values(\"Date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_model_features(df):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with a fixed set of features.\n",
    "    Required features:\n",
    "      - Base: Open, High, Low, Close, Volume\n",
    "      - Indicators: RSI_14,\n",
    "                    SMA_14, SMA_26, SMA_50, SMA_100, SMA_200,\n",
    "                    EMA_14, EMA_26, EMA_50, EMA_100, EMA_200,\n",
    "                    Bollinger Bands: BB_Hband, BB_Mband, BB_Lband,\n",
    "                    Derived: Mean_HL\n",
    "    If any feature is missing, fill with 0.\n",
    "    \"\"\"\n",
    "    desired_features = [\n",
    "        \"Open\", \"High\", \"Low\", \"Close\", \"Volume\",\n",
    "        \"RSI_14\",\n",
    "        \"SMA_14\", \"SMA_26\", \"SMA_50\", \"SMA_100\", \"SMA_200\",\n",
    "        \"EMA_14\", \"EMA_26\", \"EMA_50\", \"EMA_100\", \"EMA_200\",\n",
    "        \"BB_Hband\", \"BB_Mband\", \"BB_Lband\",\n",
    "        \"Mean_HL\", \"MACD\", \"MACD_Signal\", \"MACD_Hist\",\n",
    "        \"RMom_14\", \"MomTEMA_14_ofs1\", \"MomTEMA_26_ofs1\", \"MomTEMA_50_ofs1\", \"MomTEMA_100_ofs1\", \"MomTEMA_200_ofs1\",\n",
    "        \"RCTEMA_14\", \"RCTEMA_26\", \"RCTEMA_50\", \"RCTEMA_100\", \"RCTEMA_200\",\n",
    "        \"MomEMA_14_ofs1\", \"MomEMA_26_ofs1\", \"MomEMA_50_ofs1\", \"MomEMA_100_ofs1\", \"MomEMA_200_ofs1\",\n",
    "        \"RTEMA_TEMA_14_50\", \"REMA_EMA_14_50\", \"RSMA_SMA_14_50\",\n",
    "        \"RVolSMA_20\"\n",
    "    ]\n",
    "    for col in desired_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    return df[[\"Date\"] + desired_features].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077f7d72-5953-4d73-a06a-c3dabfd0ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Part 3: Sequence Generation and Global Scaling\n",
    "# =======================================================\n",
    "# We use a sliding window of 20 time steps.\n",
    "sequence_length = 20\n",
    "\n",
    "def prepare_sequences(df, seq_length):\n",
    "    \"\"\"\n",
    "    Generate sequences from the DataFrame (sorted by Date) that contains the fixed set of features.\n",
    "    Each sequence has shape (seq_length, num_features) and the target is the next day's Close price.\n",
    "    Returns X, y, and seq_dates (as datetime64[ns]).\n",
    "    \"\"\"\n",
    "    dates = df[\"Date\"].values\n",
    "    df_features = df.drop(columns=[\"Date\"])\n",
    "    data_array = df_features.values\n",
    "    X, y, seq_dates = [], [], []\n",
    "    target_index = df_features.columns.get_loc(\"Close\")\n",
    "    for i in range(seq_length, len(data_array)):\n",
    "        X.append(data_array[i-seq_length:i])\n",
    "        y.append(data_array[i, target_index])\n",
    "        seq_dates.append(dates[i])\n",
    "    # Ensure the dates are returned as datetime64[ns]\n",
    "    seq_dates = np.array(seq_dates, dtype='datetime64[ns]')\n",
    "    return np.array(X), np.array(y), seq_dates\n",
    "\n",
    "X_list, y_list, dates_list, ticker_list = [], [], [], []\n",
    "all_tickers = []\n",
    "\n",
    "for filename in os.listdir(filtered_data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        ticker = filename.split(\".csv\")[0]\n",
    "        all_tickers.append(ticker)\n",
    "        filepath = os.path.join(filtered_data_dir, filename)\n",
    "        df_raw = load_csv_data(filepath)\n",
    "        df_feat = get_model_features(df_raw)\n",
    "        if len(df_feat) > sequence_length:\n",
    "            X, y, seq_dates = prepare_sequences(df_feat, sequence_length)\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            dates_list.append(seq_dates)\n",
    "            ticker_list.extend([ticker] * len(y))\n",
    "\n",
    "# Concatenate sequences from all tickers.\n",
    "X_all = np.concatenate(X_list, axis=0)\n",
    "y_all = np.concatenate(y_list, axis=0)\n",
    "dates_all = np.concatenate(dates_list, axis=0)  # Now with dtype datetime64[ns]\n",
    "num_features = X_all.shape[2]\n",
    "\n",
    "# Fit a global RobustScaler on all feature data.\n",
    "scaler = RobustScaler()\n",
    "all_data = X_all.reshape(-1, num_features)\n",
    "scaler.fit(all_data)\n",
    "\n",
    "def scale_sequences(X, scaler):\n",
    "    \"\"\"Scale each sequence using the fitted scaler.\"\"\"\n",
    "    return np.array([scaler.transform(seq) for seq in X])\n",
    "\n",
    "X_all_scaled = scale_sequences(X_all, scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e8a4e8-2f16-4c25-9bd9-907a9ee274e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 35076\n",
      "Testing samples: 3898\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =======================================================\n",
    "# Assumed Preprocessing (should be executed before this snippet)\n",
    "# =======================================================\n",
    "# For example:\n",
    "# X_all_scaled = ...  # shape: (n_samples, sequence_length, num_features)\n",
    "# y_all = ...         # shape: (n_samples,)\n",
    "# sequence_length = 20\n",
    "# num_features = X_all_scaled.shape[2]\n",
    "\n",
    "# =======================================================\n",
    "# Part 4: Train-Test Split (Random 80/20 Split)\n",
    "# =======================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all_scaled, y_all, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n",
    "\n",
    "# =======================================================\n",
    "# Setup Distribution Strategy for Parallel Processing\n",
    "# =======================================================\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# =======================================================\n",
    "# Custom Callback for Epoch Timing and Logging Analytics\n",
    "# =======================================================\n",
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_times = []\n",
    "        self.train_start_time = time.time()\n",
    "        print(\"Training started...\")\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        print(f\"Epoch {epoch+1} finished in {epoch_time:.2f} seconds. \"\n",
    "              f\"Loss: {logs.get('loss'):.4f}, Val Loss: {logs.get('val_loss'):.4f}\")\n",
    "    def on_train_end(self, logs=None):\n",
    "        total_time = time.time() - self.train_start_time\n",
    "        print(f\"Training completed in {total_time:.2f} seconds over {len(self.epoch_times)} epochs.\")\n",
    "        avg_epoch_time = np.mean(self.epoch_times)\n",
    "        print(f\"Average time per epoch: {avg_epoch_time:.2f} seconds.\")\n",
    "\n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5a9826-4b54-4c64-868c-4df79433fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperParameters, RandomSearch\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    sequence_length = 20\n",
    "    num_features = 43\n",
    "\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    \n",
    "    # Tune the Dense layer size\n",
    "    dense_units = hp.Choice(\"dense_units\", [32, 64, 128])\n",
    "    x = TimeDistributed(Dense(dense_units, activation='selu'))(inputs)\n",
    "    \n",
    "    # Tune number of GRU units\n",
    "    short_term_steps = hp.Int(\"short_term_steps\", min_value=3, max_value=10, step=1)\n",
    "    short_term = Lambda(lambda x: x[:, -short_term_steps:, :])(x)\n",
    "\n",
    "    short_units_1 = hp.Choice(\"short_units_1\", [32, 64, 128])\n",
    "    short_units_2 = hp.Choice(\"short_units_2\", [16, 32, 64])\n",
    "    short_branch = GRU(short_units_1, return_sequences=True, recurrent_dropout=0.2)(short_term)\n",
    "    short_branch = GRU(short_units_2, recurrent_dropout=0.1)(short_branch)\n",
    "\n",
    "    # Tune number of LSTM units in long-term branch\n",
    "    long_units_1 = hp.Choice(\"long_units_1\", [64, 128])\n",
    "    long_units_2 = hp.Choice(\"long_units_2\", [32, 64])\n",
    "    long_units_3 = hp.Choice(\"long_units_3\", [16, 32])\n",
    "    long_branch = LSTM(long_units_1, return_sequences=True, recurrent_dropout=0.2)(x)\n",
    "    long_branch = LSTM(long_units_2, return_sequences=True, recurrent_dropout=0.1)(long_branch)\n",
    "    long_branch = LSTM(long_units_3, recurrent_dropout=0.1)(long_branch)\n",
    "\n",
    "    # Merge\n",
    "    merged = Concatenate()([short_branch, long_branch])\n",
    "\n",
    "    dense_final = hp.Choice(\"dense_final\", [16, 32, 64])\n",
    "    dense_out = Dense(dense_final, activation='selu')(merged)\n",
    "    outputs = Dense(1)(dense_out)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Learning rate tuning\n",
    "    learning_rate = hp.Choice(\"lr\", [1e-2, 1e-3, 5e-4, 1e-4])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=Huber(delta=1.5),\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9700db-3376-48b8-ba84-164fd2d65caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ronak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='lstm_gru_forecast'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c5eda-cf66-4128-937f-5db3de7aa78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "128               |128               |dense_units\n",
      "9                 |9                 |short_term_steps\n",
      "128               |128               |short_units_1\n",
      "64                |64                |short_units_2\n",
      "128               |128               |long_units_1\n",
      "64                |64                |long_units_2\n",
      "32                |32                |long_units_3\n",
      "32                |32                |dense_final\n",
      "0.0005            |0.0005            |lr\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m 147/1754\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:29\u001b[0m 130ms/step - loss: 267.2347 - mae: 178.9048"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7d9a1-d13a-436e-8b11-33adfc841489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
