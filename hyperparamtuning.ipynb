{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1e1a7b-c382-4b0c-b56c-b57a92b64464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ta  # Install via: pip install ta\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, LSTM, Dense, Concatenate, TimeDistributed,Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097fbd61-9e71-47a4-abf6-32ef5c46fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired date range\n",
    "start_date_str = \"02/01/2018\"  # 2nd January 2003 (DD-MM-YYYY)\n",
    "end_date_str   = \"28/02/2025\"  # 28th February 2025 (DD-MM-YYYY)\n",
    "start_date_filter = pd.to_datetime(start_date_str, dayfirst=True)\n",
    "end_date_filter   = pd.to_datetime(end_date_str, dayfirst=True)\n",
    "raw_data_dir = \"./data\"           # Folder with raw CSV files (e.g., AAPL.csv, SPY.csv)\n",
    "filtered_data_dir = \"./filtered_data_lstmgru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f71e37-24d1-4b04-8020-05af80dbdc3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m os.makedirs(\u001b[43mfiltered_data_dir\u001b[49m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_tema\u001b[39m(series, window):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    Calculate the Triple Exponential Moving Average (TEMA)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    using the formula: TEMA = 3*EMA1 - 3*EMA2 + EMA3\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[33;03m          EMA3 = EMA(EMA2, window)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'filtered_data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "os.makedirs(filtered_data_dir, exist_ok=True)\n",
    "\n",
    "def calculate_tema(series, window):\n",
    "    \"\"\"\n",
    "    Calculate the Triple Exponential Moving Average (TEMA)\n",
    "    using the formula: TEMA = 3*EMA1 - 3*EMA2 + EMA3\n",
    "    where EMA1 = EMA(series, window)\n",
    "          EMA2 = EMA(EMA1, window)\n",
    "          EMA3 = EMA(EMA2, window)\n",
    "    \"\"\"\n",
    "    ema1 = ta.trend.EMAIndicator(close=series, window=window, fillna=False).ema_indicator()\n",
    "    ema2 = ta.trend.EMAIndicator(close=ema1, window=window, fillna=False).ema_indicator()\n",
    "    ema3 = ta.trend.EMAIndicator(close=ema2, window=window, fillna=False).ema_indicator()\n",
    "    tema = 3 * ema1 - 3 * ema2 + ema3\n",
    "    return tema\n",
    "\n",
    "def process_csv(file_path, filename):\n",
    "    try:\n",
    "        # Specific parsing based on filename\n",
    "        if filename.upper() == \"SPY.CSV\":\n",
    "            # SPY.csv is in DD-MM-YYYY format\n",
    "            df = pd.read_csv(file_path, parse_dates=[\"Date\"], dayfirst=True)\n",
    "        else:\n",
    "            # Other files are in YYYY-MM-DD format\n",
    "            df = pd.read_csv(file_path, parse_dates=[\"Date\"], dayfirst=False)\n",
    "\n",
    "        # Drop rows where Date could not be parsed\n",
    "        df.dropna(subset=[\"Date\"], inplace=True)\n",
    "\n",
    "        # Ensure Date column is datetime and remove timezone info\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "        # Verify we have the necessary columns\n",
    "        required_columns = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Warning: {filename} is missing columns: {missing_columns}\")\n",
    "            return None\n",
    "\n",
    "        # Print min and max dates for debugging\n",
    "        print(f\"{filename} - Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "\n",
    "        # Filter for dates between start and end dates\n",
    "        df_filtered = df[\n",
    "            (df[\"Date\"] >= start_date_filter) & \n",
    "            (df[\"Date\"] <= end_date_filter)\n",
    "        ].copy()\n",
    "        \n",
    "        if df_filtered.empty:\n",
    "            print(f\"Warning: {filename} has no data within the specified date range.\")\n",
    "            return None\n",
    "\n",
    "        # Retain only essential columns\n",
    "        df_filtered = df_filtered[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        \n",
    "        # --- Calculate Standard Technical Indicators ---\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"SMA_{window}\"] = ta.trend.SMAIndicator(close=df_filtered[\"Close\"], window=window, fillna=False).sma_indicator()\n",
    "            df_filtered[f\"EMA_{window}\"] = ta.trend.EMAIndicator(close=df_filtered[\"Close\"], window=window, fillna=False).ema_indicator()\n",
    "            df_filtered[f\"TEMA_{window}\"] = calculate_tema(df_filtered[\"Close\"], window)\n",
    "        \n",
    "        # Bollinger Bands (20-day window, std dev=2)\n",
    "        bb_indicator = ta.volatility.BollingerBands(close=df_filtered[\"Close\"], window=20, window_dev=2, fillna=False)\n",
    "        df_filtered[\"BB_Hband\"] = bb_indicator.bollinger_hband()\n",
    "        df_filtered[\"BB_Mband\"] = bb_indicator.bollinger_mavg()\n",
    "        df_filtered[\"BB_Lband\"] = bb_indicator.bollinger_lband()\n",
    "        \n",
    "        # RSI (14-day)\n",
    "        df_filtered[\"RSI_14\"] = ta.momentum.RSIIndicator(close=df_filtered[\"Close\"], window=14, fillna=False).rsi()\n",
    "        \n",
    "        # MACD: using default parameters (fast=12, slow=26, signal=9)\n",
    "        macd_indicator = ta.trend.MACD(close=df_filtered[\"Close\"], window_slow=26, window_fast=12, window_sign=9, fillna=False)\n",
    "        df_filtered[\"MACD\"] = macd_indicator.macd()\n",
    "        df_filtered[\"MACD_Signal\"] = macd_indicator.macd_signal()\n",
    "        df_filtered[\"MACD_Hist\"] = macd_indicator.macd_diff()\n",
    "        \n",
    "        # Derived feature: Mean_HL as the average of High and Low\n",
    "        df_filtered[\"Mean_HL\"] = (df_filtered[\"High\"] + df_filtered[\"Low\"]) / 2.0\n",
    "        \n",
    "        # --- Calculate Extra Relative & Trend-based Indicators ---\n",
    "        # Relative Momentum (RMom) for a 14-day window\n",
    "        df_filtered[\"RMom_14\"] = df_filtered[\"Close\"] / df_filtered[\"Close\"].shift(14)\n",
    "        \n",
    "        # MomTEMA: ratio of current TEMA to its previous value (offset = 1 day)\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"MomTEMA_{window}_ofs1\"] = df_filtered[f\"TEMA_{window}\"] / df_filtered[f\"TEMA_{window}\"].shift(1)\n",
    "        \n",
    "        # RCTEMA: ratio of current Close to TEMA for each window\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"RCTEMA_{window}\"] = df_filtered[\"Close\"] / df_filtered[f\"TEMA_{window}\"]\n",
    "        \n",
    "        # MomEMA: ratio of current EMA to its previous value (offset = 1 day)\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"MomEMA_{window}_ofs1\"] = df_filtered[f\"EMA_{window}\"] / df_filtered[f\"EMA_{window}\"].shift(1)\n",
    "        \n",
    "        # Ratio Indicators for a chosen fast/slow pair (example: 14 vs. 50)\n",
    "        df_filtered[\"RTEMA_TEMA_14_50\"] = df_filtered[\"TEMA_14\"] / df_filtered[\"TEMA_50\"]\n",
    "        df_filtered[\"REMA_EMA_14_50\"] = df_filtered[\"EMA_14\"] / df_filtered[\"EMA_50\"]\n",
    "        df_filtered[\"RSMA_SMA_14_50\"] = df_filtered[\"SMA_14\"] / df_filtered[\"SMA_50\"]\n",
    "        \n",
    "        # Relative Volume to SMA: compares current volume to its 20-day SMA\n",
    "        df_filtered[\"RVolSMA_20\"] = df_filtered[\"Volume\"] / df_filtered[\"Volume\"].rolling(window=20).mean()\n",
    "        \n",
    "        # Drop rows with NaN values from indicator calculations\n",
    "        df_filtered.dropna(inplace=True)\n",
    "        \n",
    "        # --- Convert all dates to ISO format (YYYY-MM-DD) ---\n",
    "        df_filtered[\"Date\"] = df_filtered[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "for filename in os.listdir(raw_data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(raw_data_dir, filename)\n",
    "        \n",
    "        # Process the file\n",
    "        processed_df = process_csv(file_path, filename)\n",
    "        \n",
    "        # Save if processing was successful\n",
    "        if processed_df is not None:\n",
    "            output_path = os.path.join(filtered_data_dir, filename)\n",
    "            processed_df.to_csv(output_path, index=False)\n",
    "            print(f\"Processed {filename} and saved to {output_path}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3169056e-b560-4b5d-9349-a77f83ad44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Part 2: Load Filtered Data and Feature Engineering for Modeling\n",
    "# =======================================================\n",
    "\n",
    "def load_csv_data(filepath):\n",
    "    \"\"\"Load CSV file and parse the Date column appropriately.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Use the correct format: ISO format (YYYY-MM-DD)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "    df.sort_values(\"Date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_model_features(df):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with a fixed set of features.\n",
    "    Required features:\n",
    "      - Base: Open, High, Low, Close, Volume\n",
    "      - Indicators: RSI_14,\n",
    "                    SMA_14, SMA_26, SMA_50, SMA_100, SMA_200,\n",
    "                    EMA_14, EMA_26, EMA_50, EMA_100, EMA_200,\n",
    "                    Bollinger Bands: BB_Hband, BB_Mband, BB_Lband,\n",
    "                    Derived: Mean_HL\n",
    "    If any feature is missing, fill with 0.\n",
    "    \"\"\"\n",
    "    desired_features = [\n",
    "        \"Open\", \"High\", \"Low\", \"Close\", \"Volume\",\n",
    "        \"RSI_14\",\n",
    "        \"SMA_14\", \"SMA_26\", \"SMA_50\", \"SMA_100\", \"SMA_200\",\n",
    "        \"EMA_14\", \"EMA_26\", \"EMA_50\", \"EMA_100\", \"EMA_200\",\n",
    "        \"BB_Hband\", \"BB_Mband\", \"BB_Lband\",\n",
    "        \"Mean_HL\", \"MACD\", \"MACD_Signal\", \"MACD_Hist\",\n",
    "        \"RMom_14\", \"MomTEMA_14_ofs1\", \"MomTEMA_26_ofs1\", \"MomTEMA_50_ofs1\", \"MomTEMA_100_ofs1\", \"MomTEMA_200_ofs1\",\n",
    "        \"RCTEMA_14\", \"RCTEMA_26\", \"RCTEMA_50\", \"RCTEMA_100\", \"RCTEMA_200\",\n",
    "        \"MomEMA_14_ofs1\", \"MomEMA_26_ofs1\", \"MomEMA_50_ofs1\", \"MomEMA_100_ofs1\", \"MomEMA_200_ofs1\",\n",
    "        \"RTEMA_TEMA_14_50\", \"REMA_EMA_14_50\", \"RSMA_SMA_14_50\",\n",
    "        \"RVolSMA_20\"\n",
    "    ]\n",
    "    for col in desired_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    return df[[\"Date\"] + desired_features].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "077f7d72-5953-4d73-a06a-c3dabfd0ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Part 3: Sequence Generation and Global Scaling\n",
    "# =======================================================\n",
    "# We use a sliding window of 20 time steps.\n",
    "sequence_length = 20\n",
    "\n",
    "def prepare_sequences(df, seq_length):\n",
    "    \"\"\"\n",
    "    Generate sequences from the DataFrame (sorted by Date) that contains the fixed set of features.\n",
    "    Each sequence has shape (seq_length, num_features) and the target is the next day's Close price.\n",
    "    Returns X, y, and seq_dates (as datetime64[ns]).\n",
    "    \"\"\"\n",
    "    dates = df[\"Date\"].values\n",
    "    df_features = df.drop(columns=[\"Date\"])\n",
    "    data_array = df_features.values\n",
    "    X, y, seq_dates = [], [], []\n",
    "    target_index = df_features.columns.get_loc(\"Close\")\n",
    "    for i in range(seq_length, len(data_array)):\n",
    "        X.append(data_array[i-seq_length:i])\n",
    "        y.append(data_array[i, target_index])\n",
    "        seq_dates.append(dates[i])\n",
    "    # Ensure the dates are returned as datetime64[ns]\n",
    "    seq_dates = np.array(seq_dates, dtype='datetime64[ns]')\n",
    "    return np.array(X), np.array(y), seq_dates\n",
    "\n",
    "X_list, y_list, dates_list, ticker_list = [], [], [], []\n",
    "all_tickers = []\n",
    "\n",
    "for filename in os.listdir(filtered_data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        ticker = filename.split(\".csv\")[0]\n",
    "        all_tickers.append(ticker)\n",
    "        filepath = os.path.join(filtered_data_dir, filename)\n",
    "        df_raw = load_csv_data(filepath)\n",
    "        df_feat = get_model_features(df_raw)\n",
    "        if len(df_feat) > sequence_length:\n",
    "            X, y, seq_dates = prepare_sequences(df_feat, sequence_length)\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            dates_list.append(seq_dates)\n",
    "            ticker_list.extend([ticker] * len(y))\n",
    "\n",
    "# Concatenate sequences from all tickers.\n",
    "X_all = np.concatenate(X_list, axis=0)\n",
    "y_all = np.concatenate(y_list, axis=0)\n",
    "dates_all = np.concatenate(dates_list, axis=0)  # Now with dtype datetime64[ns]\n",
    "num_features = X_all.shape[2]\n",
    "\n",
    "# Fit a global RobustScaler on all feature data.\n",
    "scaler = RobustScaler()\n",
    "all_data = X_all.reshape(-1, num_features)\n",
    "scaler.fit(all_data)\n",
    "\n",
    "def scale_sequences(X, scaler):\n",
    "    \"\"\"Scale each sequence using the fitted scaler.\"\"\"\n",
    "    return np.array([scaler.transform(seq) for seq in X])\n",
    "\n",
    "X_all_scaled = scale_sequences(X_all, scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63e8a4e8-2f16-4c25-9bd9-907a9ee274e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 35076\n",
      "Testing samples: 3898\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =======================================================\n",
    "# Assumed Preprocessing (should be executed before this snippet)\n",
    "# =======================================================\n",
    "# For example:\n",
    "# X_all_scaled = ...  # shape: (n_samples, sequence_length, num_features)\n",
    "# y_all = ...         # shape: (n_samples,)\n",
    "# sequence_length = 20\n",
    "# num_features = X_all_scaled.shape[2]\n",
    "\n",
    "# =======================================================\n",
    "# Part 4: Train-Test Split (Random 80/20 Split)\n",
    "# =======================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all_scaled, y_all, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n",
    "\n",
    "# =======================================================\n",
    "# Setup Distribution Strategy for Parallel Processing\n",
    "# =======================================================\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# =======================================================\n",
    "# Custom Callback for Epoch Timing and Logging Analytics\n",
    "# =======================================================\n",
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_times = []\n",
    "        self.train_start_time = time.time()\n",
    "        print(\"Training started...\")\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        print(f\"Epoch {epoch+1} finished in {epoch_time:.2f} seconds. \"\n",
    "              f\"Loss: {logs.get('loss'):.4f}, Val Loss: {logs.get('val_loss'):.4f}\")\n",
    "    def on_train_end(self, logs=None):\n",
    "        total_time = time.time() - self.train_start_time\n",
    "        print(f\"Training completed in {total_time:.2f} seconds over {len(self.epoch_times)} epochs.\")\n",
    "        avg_epoch_time = np.mean(self.epoch_times)\n",
    "        print(f\"Average time per epoch: {avg_epoch_time:.2f} seconds.\")\n",
    "\n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e5a9826-4b54-4c64-868c-4df79433fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperParameters, RandomSearch\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    sequence_length = 20\n",
    "    num_features = 43\n",
    "\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    \n",
    "    # Tune the Dense layer size\n",
    "    dense_units = hp.Choice(\"dense_units\", [32, 64, 128])\n",
    "    x = TimeDistributed(Dense(dense_units, activation='selu'))(inputs)\n",
    "    \n",
    "    # Tune number of GRU units\n",
    "    short_term_steps = hp.Int(\"short_term_steps\", min_value=3, max_value=10, step=1)\n",
    "    short_term = Lambda(lambda x: x[:, -short_term_steps:, :])(x)\n",
    "\n",
    "    short_units_1 = hp.Choice(\"short_units_1\", [32, 64, 128])\n",
    "    short_units_2 = hp.Choice(\"short_units_2\", [16, 32, 64])\n",
    "    short_branch = GRU(short_units_1, return_sequences=True, recurrent_dropout=0.2)(short_term)\n",
    "    short_branch = GRU(short_units_2, recurrent_dropout=0.1)(short_branch)\n",
    "\n",
    "    # Tune number of LSTM units in long-term branch\n",
    "    long_units_1 = hp.Choice(\"long_units_1\", [64, 128])\n",
    "    long_units_2 = hp.Choice(\"long_units_2\", [32, 64])\n",
    "    long_units_3 = hp.Choice(\"long_units_3\", [16, 32])\n",
    "    long_branch = LSTM(long_units_1, return_sequences=True, recurrent_dropout=0.2)(x)\n",
    "    long_branch = LSTM(long_units_2, return_sequences=True, recurrent_dropout=0.1)(long_branch)\n",
    "    long_branch = LSTM(long_units_3, recurrent_dropout=0.1)(long_branch)\n",
    "\n",
    "    # Merge\n",
    "    merged = Concatenate()([short_branch, long_branch])\n",
    "\n",
    "    dense_final = hp.Choice(\"dense_final\", [16, 32, 64])\n",
    "    dense_out = Dense(dense_final, activation='selu')(merged)\n",
    "    outputs = Dense(1)(dense_out)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Learning rate tuning\n",
    "    learning_rate = hp.Choice(\"lr\", [1e-2, 1e-3, 5e-4, 1e-4])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=Huber(delta=1.5),\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e9700db-3376-48b8-ba84-164fd2d65caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ronak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='lstm_gru_forecast'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "986c5eda-cf66-4128-937f-5db3de7aa78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 35m 27s]\n",
      "val_loss: 3.4057230949401855\n",
      "\n",
      "Best val_loss So Far: 3.4057230949401855\n",
      "Total elapsed time: 01h 19m 40s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "64                |64                |dense_units\n",
      "9                 |3                 |short_term_steps\n",
      "32                |128               |short_units_1\n",
      "64                |32                |short_units_2\n",
      "128               |64                |long_units_1\n",
      "32                |64                |long_units_2\n",
      "32                |16                |long_units_3\n",
      "16                |16                |dense_final\n",
      "0.01              |0.0005            |lr\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 30ms/step - loss: 70.6862 - mae: 47.8594 - val_loss: 18.0157 - val_mae: 12.7317\n",
      "Epoch 2/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - loss: 34.5402 - mae: 23.7533 - val_loss: 28.9518 - val_mae: 20.0377\n",
      "Epoch 3/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 42ms/step - loss: 29.9047 - mae: 20.6682 - val_loss: 25.8488 - val_mae: 17.9629\n",
      "Epoch 4/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 43ms/step - loss: 17.5791 - mae: 12.4438 - val_loss: 13.2801 - val_mae: 9.5714\n",
      "Epoch 5/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 27ms/step - loss: 13.7339 - mae: 9.8714 - val_loss: 10.6829 - val_mae: 7.8173\n",
      "Epoch 6/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 38ms/step - loss: 11.9420 - mae: 8.6689 - val_loss: 14.9454 - val_mae: 10.6756\n",
      "Epoch 7/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 34ms/step - loss: 13.1820 - mae: 9.5027 - val_loss: 11.0586 - val_mae: 8.0767\n",
      "Epoch 8/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - loss: 12.9940 - mae: 9.3746 - val_loss: 15.0813 - val_mae: 10.7646\n",
      "Epoch 9/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 32ms/step - loss: 13.2945 - mae: 9.5759 - val_loss: 10.5907 - val_mae: 7.7646\n",
      "Epoch 10/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 37ms/step - loss: 12.4157 - mae: 8.9860 - val_loss: 14.7623 - val_mae: 10.5661\n",
      "Epoch 11/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 37ms/step - loss: 11.6558 - mae: 8.4811 - val_loss: 9.0634 - val_mae: 6.7412\n",
      "Epoch 12/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 41ms/step - loss: 10.8006 - mae: 7.9062 - val_loss: 11.6961 - val_mae: 8.5041\n",
      "Epoch 13/100\n",
      "\u001b[1m1754/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 38ms/step - loss: 11.0585 - mae: 8.0767 - val_loss: 14.6894 - val_mae: 10.5062\n",
      "Epoch 14/100\n",
      "\u001b[1m1579/1754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 11.3046 - mae: 8.2436 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m early_stop = EarlyStopping(monitor=\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[39m, in \u001b[36mBaseTuner.search\u001b[39m\u001b[34m(self, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_begin(trial)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_end(trial)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.on_search_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[39m, in \u001b[36mBaseTuner._try_run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m         trial.status = trial_module.TrialStatus.COMPLETED\n\u001b[32m    276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[39m, in \u001b[36mBaseTuner._run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[32m    241\u001b[39m         \u001b[38;5;28mself\u001b[39m.oracle.objective.name\n\u001b[32m    242\u001b[39m     ):\n\u001b[32m    243\u001b[39m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[32m    244\u001b[39m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[32m    245\u001b[39m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[32m    246\u001b[39m         warnings.warn(\n\u001b[32m    247\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe use case of calling \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    255\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[39m, in \u001b[36mTuner.run_trial\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m     callbacks.append(model_checkpoint)\n\u001b[32m    313\u001b[39m     copied_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m] = callbacks\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     obj_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     histories.append(obj_value)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[39m, in \u001b[36mTuner._build_and_fit_model\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m hp = trial.hyperparameters\n\u001b[32m    232\u001b[39m model = \u001b[38;5;28mself\u001b[39m._try_build(hp)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.config.multi_backend():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[39m, in \u001b[36mHyperModel.fit\u001b[39m\u001b[34m(self, hp, model, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, *args, **kwargs):\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[32m    127\u001b[39m \n\u001b[32m    128\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[33;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1681\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1691\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1692\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1693\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1697\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1698\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7d9a1-d13a-436e-8b11-33adfc841489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
