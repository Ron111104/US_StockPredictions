{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac25e28-6fae-4459-b095-a3215cdef982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ta  # Install via: pip install ta\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, LSTM, Dense, Concatenate, TimeDistributed,Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767e3ccf-c061-4857-8796-7dc9a5e7f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired date range\n",
    "start_date_str = \"02/01/2018\"  # 2nd January 2003 (DD-MM-YYYY)\n",
    "end_date_str   = \"28/02/2025\"  # 28th February 2025 (DD-MM-YYYY)\n",
    "start_date_filter = pd.to_datetime(start_date_str, dayfirst=True)\n",
    "end_date_filter   = pd.to_datetime(end_date_str, dayfirst=True)\n",
    "raw_data_dir = \"./data\"           # Folder with raw CSV files (e.g., AAPL.csv, SPY.csv)\n",
    "filtered_data_dir = \"./filtered_data_lstmgru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e5ad1-4915-417e-b27d-ac163f22f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.makedirs(filtered_data_dir, exist_ok=True)\n",
    "\n",
    "def calculate_tema(series, window):\n",
    "    \"\"\"\n",
    "    Calculate the Triple Exponential Moving Average (TEMA)\n",
    "    using the formula: TEMA = 3*EMA1 - 3*EMA2 + EMA3\n",
    "    where EMA1 = EMA(series, window)\n",
    "          EMA2 = EMA(EMA1, window)\n",
    "          EMA3 = EMA(EMA2, window)\n",
    "    \"\"\"\n",
    "    ema1 = ta.trend.EMAIndicator(close=series, window=window, fillna=False).ema_indicator()\n",
    "    ema2 = ta.trend.EMAIndicator(close=ema1, window=window, fillna=False).ema_indicator()\n",
    "    ema3 = ta.trend.EMAIndicator(close=ema2, window=window, fillna=False).ema_indicator()\n",
    "    tema = 3 * ema1 - 3 * ema2 + ema3\n",
    "    return tema\n",
    "\n",
    "def process_csv(file_path, filename):\n",
    "    try:\n",
    "        # Specific parsing based on filename\n",
    "        if filename.upper() == \"SPY.CSV\":\n",
    "            # SPY.csv is in DD-MM-YYYY format\n",
    "            df = pd.read_csv(file_path, parse_dates=[\"Date\"], dayfirst=True)\n",
    "        else:\n",
    "            # Other files are in YYYY-MM-DD format\n",
    "            df = pd.read_csv(file_path, parse_dates=[\"Date\"], dayfirst=False)\n",
    "\n",
    "        # Drop rows where Date could not be parsed\n",
    "        df.dropna(subset=[\"Date\"], inplace=True)\n",
    "\n",
    "        # Ensure Date column is datetime and remove timezone info\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "        # Verify we have the necessary columns\n",
    "        required_columns = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Warning: {filename} is missing columns: {missing_columns}\")\n",
    "            return None\n",
    "\n",
    "        # Print min and max dates for debugging\n",
    "        print(f\"{filename} - Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "\n",
    "        # Filter for dates between start and end dates\n",
    "        df_filtered = df[\n",
    "            (df[\"Date\"] >= start_date_filter) & \n",
    "            (df[\"Date\"] <= end_date_filter)\n",
    "        ].copy()\n",
    "        \n",
    "        if df_filtered.empty:\n",
    "            print(f\"Warning: {filename} has no data within the specified date range.\")\n",
    "            return None\n",
    "\n",
    "        # Retain only essential columns\n",
    "        df_filtered = df_filtered[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        \n",
    "        # --- Calculate Standard Technical Indicators ---\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"SMA_{window}\"] = ta.trend.SMAIndicator(close=df_filtered[\"Close\"], window=window, fillna=False).sma_indicator()\n",
    "            df_filtered[f\"EMA_{window}\"] = ta.trend.EMAIndicator(close=df_filtered[\"Close\"], window=window, fillna=False).ema_indicator()\n",
    "            df_filtered[f\"TEMA_{window}\"] = calculate_tema(df_filtered[\"Close\"], window)\n",
    "        \n",
    "        # Bollinger Bands (20-day window, std dev=2)\n",
    "        bb_indicator = ta.volatility.BollingerBands(close=df_filtered[\"Close\"], window=20, window_dev=2, fillna=False)\n",
    "        df_filtered[\"BB_Hband\"] = bb_indicator.bollinger_hband()\n",
    "        df_filtered[\"BB_Mband\"] = bb_indicator.bollinger_mavg()\n",
    "        df_filtered[\"BB_Lband\"] = bb_indicator.bollinger_lband()\n",
    "        \n",
    "        # RSI (14-day)\n",
    "        df_filtered[\"RSI_14\"] = ta.momentum.RSIIndicator(close=df_filtered[\"Close\"], window=14, fillna=False).rsi()\n",
    "        \n",
    "        # MACD: using default parameters (fast=12, slow=26, signal=9)\n",
    "        macd_indicator = ta.trend.MACD(close=df_filtered[\"Close\"], window_slow=26, window_fast=12, window_sign=9, fillna=False)\n",
    "        df_filtered[\"MACD\"] = macd_indicator.macd()\n",
    "        df_filtered[\"MACD_Signal\"] = macd_indicator.macd_signal()\n",
    "        df_filtered[\"MACD_Hist\"] = macd_indicator.macd_diff()\n",
    "        \n",
    "        # Derived feature: Mean_HL as the average of High and Low\n",
    "        df_filtered[\"Mean_HL\"] = (df_filtered[\"High\"] + df_filtered[\"Low\"]) / 2.0\n",
    "        \n",
    "        # --- Calculate Extra Relative & Trend-based Indicators ---\n",
    "        # Relative Momentum (RMom) for a 14-day window\n",
    "        df_filtered[\"RMom_14\"] = df_filtered[\"Close\"] / df_filtered[\"Close\"].shift(14)\n",
    "        \n",
    "        # MomTEMA: ratio of current TEMA to its previous value (offset = 1 day)\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"MomTEMA_{window}_ofs1\"] = df_filtered[f\"TEMA_{window}\"] / df_filtered[f\"TEMA_{window}\"].shift(1)\n",
    "        \n",
    "        # RCTEMA: ratio of current Close to TEMA for each window\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"RCTEMA_{window}\"] = df_filtered[\"Close\"] / df_filtered[f\"TEMA_{window}\"]\n",
    "        \n",
    "        # MomEMA: ratio of current EMA to its previous value (offset = 1 day)\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"MomEMA_{window}_ofs1\"] = df_filtered[f\"EMA_{window}\"] / df_filtered[f\"EMA_{window}\"].shift(1)\n",
    "        \n",
    "        # Ratio Indicators for a chosen fast/slow pair (example: 14 vs. 50)\n",
    "        df_filtered[\"RTEMA_TEMA_14_50\"] = df_filtered[\"TEMA_14\"] / df_filtered[\"TEMA_50\"]\n",
    "        df_filtered[\"REMA_EMA_14_50\"] = df_filtered[\"EMA_14\"] / df_filtered[\"EMA_50\"]\n",
    "        df_filtered[\"RSMA_SMA_14_50\"] = df_filtered[\"SMA_14\"] / df_filtered[\"SMA_50\"]\n",
    "        \n",
    "        # Relative Volume to SMA: compares current volume to its 20-day SMA\n",
    "        df_filtered[\"RVolSMA_20\"] = df_filtered[\"Volume\"] / df_filtered[\"Volume\"].rolling(window=20).mean()\n",
    "        \n",
    "        # Drop rows with NaN values from indicator calculations\n",
    "        df_filtered.dropna(inplace=True)\n",
    "        \n",
    "        # --- Convert all dates to ISO format (YYYY-MM-DD) ---\n",
    "        df_filtered[\"Date\"] = df_filtered[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "for filename in os.listdir(raw_data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(raw_data_dir, filename)\n",
    "        \n",
    "        # Process the file\n",
    "        processed_df = process_csv(file_path, filename)\n",
    "        \n",
    "        # Save if processing was successful\n",
    "        if processed_df is not None:\n",
    "            output_path = os.path.join(filtered_data_dir, filename)\n",
    "            processed_df.to_csv(output_path, index=False)\n",
    "            print(f\"Processed {filename} and saved to {output_path}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59a399a-1d34-400c-9f36-ef4f7c6b1668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Part 2: Load Filtered Data and Feature Engineering for Modeling\n",
    "# =======================================================\n",
    "\n",
    "def load_csv_data(filepath):\n",
    "    \"\"\"Load CSV file and parse the Date column appropriately.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Use the correct format: ISO format (YYYY-MM-DD)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "    df.sort_values(\"Date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_model_features(df):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with a fixed set of features.\n",
    "    Required features:\n",
    "      - Base: Open, High, Low, Close, Volume\n",
    "      - Indicators: RSI_14,\n",
    "                    SMA_14, SMA_26, SMA_50, SMA_100, SMA_200,\n",
    "                    EMA_14, EMA_26, EMA_50, EMA_100, EMA_200,\n",
    "                    Bollinger Bands: BB_Hband, BB_Mband, BB_Lband,\n",
    "                    Derived: Mean_HL\n",
    "    If any feature is missing, fill with 0.\n",
    "    \"\"\"\n",
    "    desired_features = [\n",
    "        \"Open\", \"High\", \"Low\", \"Close\", \"Volume\",\n",
    "        \"RSI_14\",\n",
    "        \"SMA_14\", \"SMA_26\", \"SMA_50\", \"SMA_100\", \"SMA_200\",\n",
    "        \"EMA_14\", \"EMA_26\", \"EMA_50\", \"EMA_100\", \"EMA_200\",\n",
    "        \"BB_Hband\", \"BB_Mband\", \"BB_Lband\",\n",
    "        \"Mean_HL\", \"MACD\", \"MACD_Signal\", \"MACD_Hist\",\n",
    "        \"RMom_14\", \"MomTEMA_14_ofs1\", \"MomTEMA_26_ofs1\", \"MomTEMA_50_ofs1\", \"MomTEMA_100_ofs1\", \"MomTEMA_200_ofs1\",\n",
    "        \"RCTEMA_14\", \"RCTEMA_26\", \"RCTEMA_50\", \"RCTEMA_100\", \"RCTEMA_200\",\n",
    "        \"MomEMA_14_ofs1\", \"MomEMA_26_ofs1\", \"MomEMA_50_ofs1\", \"MomEMA_100_ofs1\", \"MomEMA_200_ofs1\",\n",
    "        \"RTEMA_TEMA_14_50\", \"REMA_EMA_14_50\", \"RSMA_SMA_14_50\",\n",
    "        \"RVolSMA_20\"\n",
    "    ]\n",
    "    for col in desired_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    return df[[\"Date\"] + desired_features].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f51ab4-2674-4baf-9d46-6fd9a77ce5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Part 3: Sequence Generation and Global Scaling\n",
    "# =======================================================\n",
    "# We use a sliding window of 20 time steps.\n",
    "sequence_length = 20\n",
    "\n",
    "def prepare_sequences(df, seq_length):\n",
    "    \"\"\"\n",
    "    Generate sequences from the DataFrame (sorted by Date) that contains the fixed set of features.\n",
    "    Each sequence has shape (seq_length, num_features) and the target is the next day's Close price.\n",
    "    Returns X, y, and seq_dates (as datetime64[ns]).\n",
    "    \"\"\"\n",
    "    dates = df[\"Date\"].values\n",
    "    df_features = df.drop(columns=[\"Date\"])\n",
    "    data_array = df_features.values\n",
    "    X, y, seq_dates = [], [], []\n",
    "    target_index = df_features.columns.get_loc(\"Close\")\n",
    "    for i in range(seq_length, len(data_array)):\n",
    "        X.append(data_array[i-seq_length:i])\n",
    "        y.append(data_array[i, target_index])\n",
    "        seq_dates.append(dates[i])\n",
    "    # Ensure the dates are returned as datetime64[ns]\n",
    "    seq_dates = np.array(seq_dates, dtype='datetime64[ns]')\n",
    "    return np.array(X), np.array(y), seq_dates\n",
    "\n",
    "X_list, y_list, dates_list, ticker_list = [], [], [], []\n",
    "all_tickers = []\n",
    "\n",
    "for filename in os.listdir(filtered_data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        ticker = filename.split(\".csv\")[0]\n",
    "        all_tickers.append(ticker)\n",
    "        filepath = os.path.join(filtered_data_dir, filename)\n",
    "        df_raw = load_csv_data(filepath)\n",
    "        df_feat = get_model_features(df_raw)\n",
    "        if len(df_feat) > sequence_length:\n",
    "            X, y, seq_dates = prepare_sequences(df_feat, sequence_length)\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            dates_list.append(seq_dates)\n",
    "            ticker_list.extend([ticker] * len(y))\n",
    "\n",
    "# Concatenate sequences from all tickers.\n",
    "X_all = np.concatenate(X_list, axis=0)\n",
    "y_all = np.concatenate(y_list, axis=0)\n",
    "dates_all = np.concatenate(dates_list, axis=0)  # Now with dtype datetime64[ns]\n",
    "num_features = X_all.shape[2]\n",
    "\n",
    "# Fit a global RobustScaler on all feature data.\n",
    "scaler = RobustScaler()\n",
    "all_data = X_all.reshape(-1, num_features)\n",
    "scaler.fit(all_data)\n",
    "\n",
    "def scale_sequences(X, scaler):\n",
    "    \"\"\"Scale each sequence using the fitted scaler.\"\"\"\n",
    "    return np.array([scaler.transform(seq) for seq in X])\n",
    "\n",
    "X_all_scaled = scale_sequences(X_all, scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a11159-80b9-4b2b-ad17-4721b1cd9cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 31179\n",
      "Testing samples: 7795\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =======================================================\n",
    "# Assumed Preprocessing (should be executed before this snippet)\n",
    "# =======================================================\n",
    "# For example:\n",
    "# X_all_scaled = ...  # shape: (n_samples, sequence_length, num_features)\n",
    "# y_all = ...         # shape: (n_samples,)\n",
    "# sequence_length = 20\n",
    "# num_features = X_all_scaled.shape[2]\n",
    "\n",
    "# =======================================================\n",
    "# Part 4: Train-Test Split (Random 80/20 Split)\n",
    "# =======================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all_scaled, y_all, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n",
    "\n",
    "# =======================================================\n",
    "# Setup Distribution Strategy for Parallel Processing\n",
    "# =======================================================\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# =======================================================\n",
    "# Custom Callback for Epoch Timing and Logging Analytics\n",
    "# =======================================================\n",
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_times = []\n",
    "        self.train_start_time = time.time()\n",
    "        print(\"Training started...\")\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        print(f\"Epoch {epoch+1} finished in {epoch_time:.2f} seconds. \"\n",
    "              f\"Loss: {logs.get('loss'):.4f}, Val Loss: {logs.get('val_loss'):.4f}\")\n",
    "    def on_train_end(self, logs=None):\n",
    "        total_time = time.time() - self.train_start_time\n",
    "        print(f\"Training completed in {total_time:.2f} seconds over {len(self.epoch_times)} epochs.\")\n",
    "        avg_epoch_time = np.mean(self.epoch_times)\n",
    "        print(f\"Average time per epoch: {avg_epoch_time:.2f} seconds.\")\n",
    "\n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907cf4a-7822-43c3-bf54-68b5ecbf6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example settings: update these based on your data.\n",
    "sequence_length = 20   # total number of timesteps per sequence\n",
    "num_features = 43      # number of features per timestep\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "with strategy.scope():\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    \n",
    "    # Feature extraction for the entire sequence\n",
    "    x = TimeDistributed(Dense(64, activation='selu'))(inputs)\n",
    "    \n",
    "    # Short-term branch: process only the last few timesteps (e.g., last 5)\n",
    "    short_term = Lambda(lambda x: x[:, -5:, :])(x)\n",
    "    short_branch = GRU(64, return_sequences=True, recurrent_dropout=0.2)(short_term)\n",
    "    short_branch = GRU(32, recurrent_dropout=0.1)(short_branch)\n",
    "    \n",
    "    # Long-term branch: process the full sequence with an extra hidden LSTM layer\n",
    "    long_branch = LSTM(128, return_sequences=True, recurrent_dropout=0.2)(x)\n",
    "    long_branch = LSTM(64, return_sequences=True, recurrent_dropout=0.1)(long_branch)\n",
    "    long_branch = LSTM(32, recurrent_dropout=0.1)(long_branch)  # Extra hidden layer for better long-term processing\n",
    "    \n",
    "    # Merge the outputs from both branches\n",
    "    merged = Concatenate()([short_branch, long_branch])\n",
    "    \n",
    "    # Final Dense layers for prediction\n",
    "    dense_out = Dense(32, activation='selu')(merged)\n",
    "    outputs = Dense(1)(dense_out)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=5e-4),\n",
    "                  loss=Huber(delta=1.5),\n",
    "                  metrics=[\"mae\"])\n",
    "\n",
    "    \n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test, and time_callback are defined elsewhere\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, time_callback]\n",
    ")\n",
    "\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test MAE:\", mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
