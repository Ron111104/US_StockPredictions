{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99cb3eae-508c-4e3b-8c9c-94ccfcc4980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ta  # Install via: pip install ta\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, LSTM, Dense, Concatenate, TimeDistributed, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b972111a-c4c2-4642-b205-6794c004cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# PART 1: Data Processing & Feature Engineering\n",
    "# =======================================================\n",
    "\n",
    "# Desired date range (modify as needed)\n",
    "start_date_str = \"02/01/2018\"  # dayfirst format\n",
    "end_date_str   = \"28/02/2025\"\n",
    "start_date_filter = pd.to_datetime(start_date_str, dayfirst=True)\n",
    "end_date_filter   = pd.to_datetime(end_date_str, dayfirst=True)\n",
    "raw_data_dir = \"./data\"           # Folder with raw CSV files (e.g., AAPL.csv, SPY.csv)\n",
    "filtered_data_dir = \"./filtered_data_optimizations\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db83014-1176-4591-abb2-6bc0a50a401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL.csv - Date Range: 1980-12-12 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed AAPL.csv -> ./filtered_data_optimizations\\AAPL.csv\n",
      "ABBV.csv - Date Range: 2013-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed ABBV.csv -> ./filtered_data_optimizations\\ABBV.csv\n",
      "ADBE.csv - Date Range: 1986-08-13 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed ADBE.csv -> ./filtered_data_optimizations\\ADBE.csv\n",
      "AMD.csv - Date Range: 1980-03-17 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed AMD.csv -> ./filtered_data_optimizations\\AMD.csv\n",
      "AMT.csv - Date Range: 1998-02-27 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed AMT.csv -> ./filtered_data_optimizations\\AMT.csv\n",
      "AMZN.csv - Date Range: 1997-05-15 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed AMZN.csv -> ./filtered_data_optimizations\\AMZN.csv\n",
      "BA.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed BA.csv -> ./filtered_data_optimizations\\BA.csv\n",
      "BAC.csv - Date Range: 1973-02-21 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed BAC.csv -> ./filtered_data_optimizations\\BAC.csv\n",
      "BLK.csv - Date Range: 1999-10-01 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed BLK.csv -> ./filtered_data_optimizations\\BLK.csv\n",
      "CAT.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed CAT.csv -> ./filtered_data_optimizations\\CAT.csv\n",
      "COP.csv - Date Range: 1981-12-31 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed COP.csv -> ./filtered_data_optimizations\\COP.csv\n",
      "CRM.csv - Date Range: 2004-06-23 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed CRM.csv -> ./filtered_data_optimizations\\CRM.csv\n",
      "CVX.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed CVX.csv -> ./filtered_data_optimizations\\CVX.csv\n",
      "DUK.csv - Date Range: 1980-03-17 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed DUK.csv -> ./filtered_data_optimizations\\DUK.csv\n",
      "FCX.csv - Date Range: 1995-07-10 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed FCX.csv -> ./filtered_data_optimizations\\FCX.csv\n",
      "GOOGL.csv - Date Range: 2004-08-19 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed GOOGL.csv -> ./filtered_data_optimizations\\GOOGL.csv\n",
      "GS.csv - Date Range: 1999-05-04 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed GS.csv -> ./filtered_data_optimizations\\GS.csv\n",
      "HD.csv - Date Range: 1981-09-22 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed HD.csv -> ./filtered_data_optimizations\\HD.csv\n",
      "HON.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed HON.csv -> ./filtered_data_optimizations\\HON.csv\n",
      "JNJ.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed JNJ.csv -> ./filtered_data_optimizations\\JNJ.csv\n",
      "JPM.csv - Date Range: 1980-03-17 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed JPM.csv -> ./filtered_data_optimizations\\JPM.csv\n",
      "LIN.csv - Date Range: 1992-06-17 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed LIN.csv -> ./filtered_data_optimizations\\LIN.csv\n",
      "META.csv - Date Range: 2012-05-18 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed META.csv -> ./filtered_data_optimizations\\META.csv\n",
      "MRK.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed MRK.csv -> ./filtered_data_optimizations\\MRK.csv\n",
      "MSFT.csv - Date Range: 1986-03-13 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed MSFT.csv -> ./filtered_data_optimizations\\MSFT.csv\n",
      "NEE.csv - Date Range: 1973-02-21 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed NEE.csv -> ./filtered_data_optimizations\\NEE.csv\n",
      "NVDA.csv - Date Range: 1999-01-22 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed NVDA.csv -> ./filtered_data_optimizations\\NVDA.csv\n",
      "PFE.csv - Date Range: 1972-06-01 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed PFE.csv -> ./filtered_data_optimizations\\PFE.csv\n",
      "PLD.csv - Date Range: 1997-11-21 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed PLD.csv -> ./filtered_data_optimizations\\PLD.csv\n",
      "SBUX.csv - Date Range: 1992-06-26 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed SBUX.csv -> ./filtered_data_optimizations\\SBUX.csv\n",
      "SPY.csv - Date Range: 1993-01-29 00:00:00 to 2025-02-28 00:00:00\n",
      "Processed SPY.csv -> ./filtered_data_optimizations\\SPY.csv\n",
      "TSLA.csv - Date Range: 2010-06-29 04:00:00 to 2025-03-03 05:00:00\n",
      "Processed TSLA.csv -> ./filtered_data_optimizations\\TSLA.csv\n",
      "XOM.csv - Date Range: 1962-01-02 05:00:00 to 2025-03-03 05:00:00\n",
      "Processed XOM.csv -> ./filtered_data_optimizations\\XOM.csv\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(filtered_data_dir, exist_ok=True)\n",
    "\n",
    "def calculate_tema(series, window):\n",
    "    \"\"\"\n",
    "    Calculate Triple Exponential Moving Average (TEMA):\n",
    "    TEMA = 3*EMA1 - 3*EMA2 + EMA3\n",
    "    \"\"\"\n",
    "    ema1 = ta.trend.EMAIndicator(close=series, window=window, fillna=False).ema_indicator()\n",
    "    ema2 = ta.trend.EMAIndicator(close=ema1, window=window, fillna=False).ema_indicator()\n",
    "    ema3 = ta.trend.EMAIndicator(close=ema2, window=window, fillna=False).ema_indicator()\n",
    "    return 3 * ema1 - 3 * ema2 + ema3\n",
    "\n",
    "def process_csv(file_path, filename):\n",
    "    try:\n",
    "        if filename.upper() == \"SPY.CSV\":\n",
    "            df = pd.read_csv(file_path, parse_dates=[\"Date\"], dayfirst=True)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, parse_dates=[\"Date\"], dayfirst=False)\n",
    "        df.dropna(subset=[\"Date\"], inplace=True)\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "        required_columns = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Warning: {filename} missing {missing_columns}\")\n",
    "            return None\n",
    "        print(f\"{filename} - Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "        df_filtered = df[(df[\"Date\"] >= start_date_filter) & (df[\"Date\"] <= end_date_filter)].copy()\n",
    "        if df_filtered.empty:\n",
    "            print(f\"Warning: {filename} has no data in the date range.\")\n",
    "            return None\n",
    "        df_filtered = df_filtered[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        # Standard Technical Indicators\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"SMA_{window}\"] = ta.trend.SMAIndicator(close=df_filtered[\"Close\"], window=window, fillna=False).sma_indicator()\n",
    "            df_filtered[f\"EMA_{window}\"] = ta.trend.EMAIndicator(close=df_filtered[\"Close\"], window=window, fillna=False).ema_indicator()\n",
    "            df_filtered[f\"TEMA_{window}\"] = calculate_tema(df_filtered[\"Close\"], window)\n",
    "        bb_indicator = ta.volatility.BollingerBands(close=df_filtered[\"Close\"], window=20, window_dev=2, fillna=False)\n",
    "        df_filtered[\"BB_Hband\"] = bb_indicator.bollinger_hband()\n",
    "        df_filtered[\"BB_Mband\"] = bb_indicator.bollinger_mavg()\n",
    "        df_filtered[\"BB_Lband\"] = bb_indicator.bollinger_lband()\n",
    "        df_filtered[\"RSI_14\"] = ta.momentum.RSIIndicator(close=df_filtered[\"Close\"], window=14, fillna=False).rsi()\n",
    "        macd_indicator = ta.trend.MACD(close=df_filtered[\"Close\"], window_slow=26, window_fast=12, window_sign=9, fillna=False)\n",
    "        df_filtered[\"MACD\"] = macd_indicator.macd()\n",
    "        df_filtered[\"MACD_Signal\"] = macd_indicator.macd_signal()\n",
    "        df_filtered[\"MACD_Hist\"] = macd_indicator.macd_diff()\n",
    "        df_filtered[\"Mean_HL\"] = (df_filtered[\"High\"] + df_filtered[\"Low\"]) / 2.0\n",
    "        df_filtered[\"RMom_14\"] = df_filtered[\"Close\"] / df_filtered[\"Close\"].shift(14)\n",
    "        for window in [14, 26, 50, 100, 200]:\n",
    "            df_filtered[f\"MomTEMA_{window}_ofs1\"] = df_filtered[f\"TEMA_{window}\"] / df_filtered[f\"TEMA_{window}\"].shift(1)\n",
    "            df_filtered[f\"RCTEMA_{window}\"] = df_filtered[\"Close\"] / df_filtered[f\"TEMA_{window}\"]\n",
    "            df_filtered[f\"MomEMA_{window}_ofs1\"] = df_filtered[f\"EMA_{window}\"] / df_filtered[f\"EMA_{window}\"].shift(1)\n",
    "        df_filtered[\"RTEMA_TEMA_14_50\"] = df_filtered[\"TEMA_14\"] / df_filtered[\"TEMA_50\"]\n",
    "        df_filtered[\"REMA_EMA_14_50\"] = df_filtered[\"EMA_14\"] / df_filtered[\"EMA_50\"]\n",
    "        df_filtered[\"RSMA_SMA_14_50\"] = df_filtered[\"SMA_14\"] / df_filtered[\"SMA_50\"]\n",
    "        df_filtered[\"RVolSMA_20\"] = df_filtered[\"Volume\"] / df_filtered[\"Volume\"].rolling(window=20).mean()\n",
    "        df_filtered.dropna(inplace=True)\n",
    "        df_filtered[\"Date\"] = df_filtered[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        return df_filtered\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process all CSV files in raw_data_dir\n",
    "for filename in os.listdir(raw_data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(raw_data_dir, filename)\n",
    "        processed_df = process_csv(file_path, filename)\n",
    "        if processed_df is not None:\n",
    "            output_path = os.path.join(filtered_data_dir, filename)\n",
    "            processed_df.to_csv(output_path, index=False)\n",
    "            print(f\"Processed {filename} -> {output_path}\")\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627b3a25-aae4-4bc3-9f53-2dbf82e58a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# PART 2: Load Filtered Data & Prepare Sequences\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def load_csv_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "    df.sort_values(\"Date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_model_features(df):\n",
    "    desired_features = [\n",
    "        \"Open\", \"High\", \"Low\", \"Close\", \"Volume\",\n",
    "        \"RSI_14\",\n",
    "        \"SMA_14\", \"SMA_26\", \"SMA_50\", \"SMA_100\", \"SMA_200\",\n",
    "        \"EMA_14\", \"EMA_26\", \"EMA_50\", \"EMA_100\", \"EMA_200\",\n",
    "        \"BB_Hband\", \"BB_Mband\", \"BB_Lband\",\n",
    "        \"Mean_HL\", \"MACD\", \"MACD_Signal\", \"MACD_Hist\",\n",
    "        \"RMom_14\", \"MomTEMA_14_ofs1\", \"MomTEMA_26_ofs1\", \"MomTEMA_50_ofs1\", \"MomTEMA_100_ofs1\", \"MomTEMA_200_ofs1\",\n",
    "        \"RCTEMA_14\", \"RCTEMA_26\", \"RCTEMA_50\", \"RCTEMA_100\", \"RCTEMA_200\",\n",
    "        \"MomEMA_14_ofs1\", \"MomEMA_26_ofs1\", \"MomEMA_50_ofs1\", \"MomEMA_100_ofs1\", \"MomEMA_200_ofs1\",\n",
    "        \"RTEMA_TEMA_14_50\", \"REMA_EMA_14_50\", \"RSMA_SMA_14_50\",\n",
    "        \"RVolSMA_20\"\n",
    "    ]\n",
    "    for col in desired_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    return df[[\"Date\"] + desired_features].copy()\n",
    "\n",
    "def prepare_sequences(df, seq_length):\n",
    "    dates = df[\"Date\"].values\n",
    "    df_features = df.drop(columns=[\"Date\"])\n",
    "    data_array = df_features.values\n",
    "    X, y, seq_dates = [], [], []\n",
    "    target_index = df_features.columns.get_loc(\"Close\")\n",
    "    for i in range(seq_length, len(data_array)):\n",
    "        X.append(data_array[i-seq_length:i])\n",
    "        y.append(data_array[i, target_index])\n",
    "        seq_dates.append(dates[i])\n",
    "    return np.array(X), np.array(y), np.array(seq_dates, dtype='datetime64[ns]')\n",
    "\n",
    "sequence_length = 20  # fixed sequence length\n",
    "\n",
    "X_list, y_list, dates_list, ticker_list = [], [], [], []\n",
    "all_tickers = []\n",
    "\n",
    "for filename in os.listdir(filtered_data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        ticker = filename.split(\".csv\")[0]\n",
    "        all_tickers.append(ticker)\n",
    "        filepath = os.path.join(filtered_data_dir, filename)\n",
    "        df_raw = load_csv_data(filepath)\n",
    "        df_feat = get_model_features(df_raw)\n",
    "        if len(df_feat) > sequence_length:\n",
    "            X, y, seq_dates = prepare_sequences(df_feat, sequence_length)\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            dates_list.append(seq_dates)\n",
    "            ticker_list.extend([ticker] * len(y))\n",
    "\n",
    "X_all = np.concatenate(X_list, axis=0)\n",
    "y_all = np.concatenate(y_list, axis=0)\n",
    "dates_all = np.concatenate(dates_list, axis=0)\n",
    "num_features = X_all.shape[2]\n",
    "\n",
    "# Global scaling using RobustScaler\n",
    "scaler = RobustScaler()\n",
    "all_data = X_all.reshape(-1, num_features)\n",
    "scaler.fit(all_data)\n",
    "def scale_sequences(X, scaler):\n",
    "    return np.array([scaler.transform(seq) for seq in X])\n",
    "X_all_scaled = scale_sequences(X_all, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a374c4-4e76-471f-b3b0-0643cf3448ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# PART 3: Walk-Forward Split Function for Sequences\n",
    "# -------------------------------------------------------\n",
    "def walk_forward_split(X, y, dates, train_window, test_window):\n",
    "    \"\"\"\n",
    "    Split sequences (X, y) into multiple folds based on the dates.\n",
    "    Data is sorted by dates before splitting.\n",
    "    \"\"\"\n",
    "    sorted_idx = np.argsort(dates)\n",
    "    X = X[sorted_idx]\n",
    "    y = y[sorted_idx]\n",
    "    dates = dates[sorted_idx]\n",
    "    splits = []\n",
    "    start_idx = 0\n",
    "    while start_idx + train_window + test_window <= len(X):\n",
    "        X_train = X[start_idx : start_idx + train_window]\n",
    "        y_train = y[start_idx : start_idx + train_window]\n",
    "        X_test = X[start_idx + train_window : start_idx + train_window + test_window]\n",
    "        y_test = y[start_idx + train_window : start_idx + train_window + test_window]\n",
    "        splits.append((X_train, y_train, X_test, y_test))\n",
    "        start_idx += test_window\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce68023-2d5d-4908-a1f3-fb4d931d4b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# PART 4: Define Distribution Strategy\n",
    "# -------------------------------------------------------\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e854be97-cab2-49c1-bcc2-291f38dd1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# PART 5: Build Model for Hyperparameter Optimization\n",
    "# -------------------------------------------------------\n",
    "def build_model_hp(params):\n",
    "    with strategy.scope():\n",
    "        inputs = Input(shape=(sequence_length, num_features))\n",
    "        # TimeDistributed feature extraction (parameterizable)\n",
    "        td_units = params.get('td_units', 64)\n",
    "        x = TimeDistributed(Dense(td_units, activation='selu'))(inputs)\n",
    "        # Short-term branch (processing last few timesteps)\n",
    "        short_timesteps = params.get('short_timesteps', 5)\n",
    "        short_term = Lambda(lambda x: x[:, -short_timesteps:, :])(x)\n",
    "        gru1_units = params.get('gru1_units', 64)\n",
    "        gru2_units = params.get('gru2_units', 32)\n",
    "        gru1_dropout = params.get('gru1_dropout', 0.2)\n",
    "        gru2_dropout = params.get('gru2_dropout', 0.1)\n",
    "        short_branch = GRU(gru1_units, return_sequences=True, recurrent_dropout=gru1_dropout)(short_term)\n",
    "        short_branch = GRU(gru2_units, recurrent_dropout=gru2_dropout)(short_branch)\n",
    "        # Long-term branch (processing full sequence)\n",
    "        lstm1_units = params.get('lstm1_units', 128)\n",
    "        lstm2_units = params.get('lstm2_units', 64)\n",
    "        lstm3_units = params.get('lstm3_units', 32)\n",
    "        lstm1_dropout = params.get('lstm1_dropout', 0.2)\n",
    "        lstm2_dropout = params.get('lstm2_dropout', 0.1)\n",
    "        lstm3_dropout = params.get('lstm3_dropout', 0.1)\n",
    "        long_branch = LSTM(lstm1_units, return_sequences=True, recurrent_dropout=lstm1_dropout)(x)\n",
    "        long_branch = LSTM(lstm2_units, return_sequences=True, recurrent_dropout=lstm2_dropout)(long_branch)\n",
    "        long_branch = LSTM(lstm3_units, recurrent_dropout=lstm3_dropout)(long_branch)\n",
    "        # Merge branches\n",
    "        merged = Concatenate()([short_branch, long_branch])\n",
    "        dense_units = params.get('dense_units', 32)\n",
    "        dense_out = Dense(dense_units, activation='selu')(merged)\n",
    "        outputs = Dense(1)(dense_out)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        lr = params.get('learning_rate', 5e-4)\n",
    "        huber_delta = params.get('huber_delta', 1.5)\n",
    "        model.compile(optimizer=Adam(learning_rate=lr),\n",
    "                      loss=Huber(delta=huber_delta),\n",
    "                      metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "373830f4-a208-4723-baf4-c6e1d09db3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# PART 6: Hyperparameter Optimization Objective Function with Monitoring\n",
    "# -------------------------------------------------------\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters for various parts of the model\n",
    "    params = {\n",
    "        'td_units': trial.suggest_int('td_units', 32, 128),\n",
    "        'short_timesteps': trial.suggest_int('short_timesteps', 3, 10),\n",
    "        'gru1_units': trial.suggest_int('gru1_units', 32, 128),\n",
    "        'gru2_units': trial.suggest_int('gru2_units', 16, 64),\n",
    "        'gru1_dropout': trial.suggest_float('gru1_dropout', 0.1, 0.4),\n",
    "        'gru2_dropout': trial.suggest_float('gru2_dropout', 0.05, 0.3),\n",
    "        'lstm1_units': trial.suggest_int('lstm1_units', 64, 256),\n",
    "        'lstm2_units': trial.suggest_int('lstm2_units', 32, 128),\n",
    "        'lstm3_units': trial.suggest_int('lstm3_units', 16, 64),\n",
    "        'lstm1_dropout': trial.suggest_float('lstm1_dropout', 0.1, 0.4),\n",
    "        'lstm2_dropout': trial.suggest_float('lstm2_dropout', 0.05, 0.3),\n",
    "        'lstm3_dropout': trial.suggest_float('lstm3_dropout', 0.05, 0.3),\n",
    "        'dense_units': trial.suggest_int('dense_units', 16, 64),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'huber_delta': trial.suggest_float('huber_delta', 1.0, 2.0)\n",
    "    }\n",
    "    \n",
    "    # Set training parameters for each fold during optimization\n",
    "    epochs = 20\n",
    "    batch_size = 16\n",
    "    \n",
    "    # Determine train and test window sizes based on total number of samples\n",
    "    N = X_all_scaled.shape[0]\n",
    "    train_window = int(0.6 * N)\n",
    "    test_window  = int(0.1 * N)\n",
    "    \n",
    "    splits = walk_forward_split(X_all_scaled, y_all, dates_all, train_window, test_window)\n",
    "    # If no valid split is obtained, fallback to one simple split.\n",
    "    if not splits:\n",
    "        X_train_fold = X_all_scaled[:int(0.8 * N)]\n",
    "        y_train_fold = y_all[:int(0.8 * N)]\n",
    "        X_test_fold = X_all_scaled[int(0.8 * N):]\n",
    "        y_test_fold = y_all[int(0.8 * N):]\n",
    "        splits = [(X_train_fold, y_train_fold, X_test_fold, y_test_fold)]\n",
    "    \n",
    "    fold_losses = []\n",
    "    print(f\"Trial {trial.number}: Starting evaluation over {len(splits)} fold(s)...\")\n",
    "    for fold_index, (X_train_fold, y_train_fold, X_test_fold, y_test_fold) in enumerate(splits, start=1):\n",
    "        print(f\"Trial {trial.number}: Processing fold {fold_index}/{len(splits)}...\")\n",
    "        model = build_model_hp(params)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        history = model.fit(\n",
    "            X_train_fold, y_train_fold, \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test_fold, y_test_fold),\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        loss, _ = model.evaluate(X_test_fold, y_test_fold, verbose=0)\n",
    "        print(f\"Trial {trial.number}, Fold {fold_index}: Loss = {loss:.4f}\")\n",
    "        fold_losses.append(loss)\n",
    "    \n",
    "    mean_loss = np.mean(fold_losses)\n",
    "    print(f\"Trial {trial.number}: Mean Loss = {mean_loss:.4f}\")\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323654ee-9bd4-40be-802e-60a990d1e72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 22:15:42,451] A new study created in memory with name: no-name-ec6357cf-a3f0-42c1-886c-de76a1d2c26d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc84fd14eef54626bbd1cd59837b67c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: Starting evaluation over 4 fold(s)...\n",
      "Trial 0: Processing fold 1/4...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# PART 7: Run Hyperparameter Optimization Study with Progress Bar\n",
    "# -------------------------------------------------------\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebde37-8136-4575-b167-3420477daa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# PART 8: Rebuild Final Model and Train on Full Training Data\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# (Option 1) Use a final train-test split based on time ordering.\n",
    "sorted_idx = np.argsort(dates_all)\n",
    "X_all_sorted = X_all_scaled[sorted_idx]\n",
    "y_all_sorted = y_all[sorted_idx]\n",
    "N = X_all_sorted.shape[0]\n",
    "train_end = int(0.8 * N)\n",
    "X_train_final = X_all_sorted[:train_end]\n",
    "y_train_final = y_all_sorted[:train_end]\n",
    "X_test_final  = X_all_sorted[train_end:]\n",
    "y_test_final  = y_all_sorted[train_end:]\n",
    "\n",
    "final_params = study.best_params  # Use best hyperparameters from Optuna\n",
    "\n",
    "final_model = build_model_hp(final_params)\n",
    "final_early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "history = final_model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[final_early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "loss, mae = final_model.evaluate(X_test_final, y_test_final)\n",
    "print(\"Final Test Loss:\", loss)\n",
    "print(\"Final Test MAE:\", mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
